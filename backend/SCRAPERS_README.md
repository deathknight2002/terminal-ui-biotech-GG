# Biotech Terminal Backend - Expanded Scraper System

## ğŸ‰ What's New

The backend has been **massively upgraded** with 5 new biotech news scrapers, bringing the total to **8 comprehensive data sources** for real-time biotech and pharmaceutical intelligence.

## ğŸ“Š Data Sources

### Medical & Research (Existing)
- âœ… **PubMed** - Medical literature and research articles
- âœ… **FDA** - Drug approvals, adverse events, recalls
- âœ… **ClinicalTrials.gov** - Clinical trial information

### Industry News (NEW! ğŸ†•)
- ğŸ”¥ **Fierce Biotech** - Biotech industry news and updates
- ğŸ”¬ **Science Daily** - Scientific research news (RSS)
- ğŸ§¬ **BioSpace** - Pharmaceutical news and job postings
- ğŸ“° **Endpoints News** - FDA approvals, clinical trials, M&A
- ğŸ’Š **BioPharma Dive** - Pipeline updates and industry insights (RSS)

## ğŸš€ Quick Start

### Installation
```bash
cd backend
npm install
```

### Start Development Server
```bash
npm run dev
```

### Test the New Scrapers
```bash
# Run example test script
npm run tsx examples/test-news-scrapers.ts

# Or test via API
curl http://localhost:3001/api/scraping/news/all?limit=5
```

## ğŸ“š Documentation

- **[NEWS_SCRAPER_API.md](./NEWS_SCRAPER_API.md)** - Complete API reference with examples
- **[SCRAPING_README.md](./src/scraping/SCRAPING_README.md)** - Architecture and deployment guide
- **[SCRAPING_INFRASTRUCTURE.md](../SCRAPING_INFRASTRUCTURE.md)** - System design overview

## ğŸ”— API Endpoints

### Aggregated News
```bash
GET /api/scraping/news/all?limit=10
```
Returns news from all 5 sources in a single request.

### Individual Sources
```bash
# Fierce Biotech
GET /api/scraping/news/fierce-biotech/latest?limit=20
POST /api/scraping/news/fierce-biotech/search

# Science Daily
GET /api/scraping/news/science-daily/latest?limit=20
GET /api/scraping/news/science-daily/category/biotechnology

# BioSpace
GET /api/scraping/news/biospace/latest?limit=20
POST /api/scraping/news/biospace/search

# Endpoints News
GET /api/scraping/news/endpoints/latest?limit=20
GET /api/scraping/news/endpoints/fda
GET /api/scraping/news/endpoints/clinical-trials

# BioPharma Dive
GET /api/scraping/news/biopharma-dive/latest?limit=20
GET /api/scraping/news/biopharma-dive/pipeline
GET /api/scraping/news/biopharma-dive/ma
```

### System Health & Monitoring
```bash
GET /api/scraping/health       # Health status of all scrapers
GET /api/scraping/stats         # Detailed statistics
GET /api/scraping/metrics       # Prometheus metrics
POST /api/scraping/cache/clear  # Clear all caches
```

## âš¡ Features

### Robustness
- âœ… Circuit breakers prevent cascade failures
- âœ… Automatic retry with exponential backoff
- âœ… Graceful degradation on partial failures
- âœ… Health checks and self-healing
- âœ… Request timeout handling

### Performance
- âœ… Aggressive caching (1-4 hour TTL)
- âœ… Parallel request execution
- âœ… Worker pool for CPU-intensive tasks
- âœ… Connection pooling
- âœ… Streaming data processing

### Compliance
- âœ… Conservative rate limiting (0.25-1 req/s)
- âœ… Respects robots.txt policies
- âœ… User-Agent identification
- âœ… No credential requirements
- âœ… Ethical scraping practices

### Monitoring
- âœ… Real-time health status
- âœ… Prometheus metrics export
- âœ… Cache hit/miss statistics
- âœ… Rate limiter performance tracking
- âœ… Circuit breaker state monitoring

## ğŸ› ï¸ Usage Example

```typescript
import { initializeScrapingManager } from './src/scraping';

// Initialize
const manager = await initializeScrapingManager();

// Get latest biotech news from Fierce Biotech
const fierceBiotech = manager.getFierceBiotechScraper();
const news = await fierceBiotech.getLatestNews(20);

// Get science research from Science Daily
const scienceDaily = manager.getScienceDailyScraper();
const research = await scienceDaily.getLatestBiotechNews(20);

// Aggregate news from all sources
const [fierce, science, bio, end, pharma] = await Promise.all([
  fierceBiotech.getLatestNews(10),
  scienceDaily.getLatestBiotechNews(10),
  manager.getBioSpaceScraper().getLatestNews(10),
  manager.getEndpointsNewsScraper().getLatestNews(10),
  manager.getBioPharmDiveScraper().getLatestNews(10),
]);
```

## ğŸ“ˆ Rate Limits

All scrapers implement adaptive rate limiting:

| Source | Rate | Adaptive |
|--------|------|----------|
| Fierce Biotech | 0.5 req/s | âœ… |
| Science Daily | 1 req/s | âœ… |
| BioSpace | 0.5 req/s | âœ… |
| Endpoints News | 0.5 req/s | âœ… |
| BioPharma Dive | 0.5 req/s | âœ… |
| PubMed | 3-10 req/s | âœ… |
| FDA | 40-240 req/min | âœ… |
| ClinicalTrials | 10 req/s | âœ… |

## ğŸ§ª Testing

Run the example test script:
```bash
npm run tsx examples/test-news-scrapers.ts
```

Expected output:
```
ğŸš€ Starting scraper tests...
âœ… Scraping Manager initialized

ğŸ“° Testing Fierce Biotech scraper...
   âœ“ Fetched 5 articles
   Sample: "Pfizer advances CAR-T therapy..."

ğŸ”¬ Testing Science Daily scraper...
   âœ“ Fetched 5 articles
   Sample: "New CRISPR technique..."

[... more tests ...]

âœ… All tests completed!
```

## ğŸ”§ Configuration

### Environment Variables
```bash
# Optional API keys (enhances rate limits)
PUBMED_API_KEY=your_ncbi_api_key
FDA_API_KEY=your_fda_api_key

# Server
API_PORT=3001
NODE_ENV=development

# Cache
CACHE_TTL=3600
CACHE_MAX_SIZE=1000
```

## ğŸ“¦ Production Deployment

### Build
```bash
npm run build
```

### Start
```bash
NODE_ENV=production npm start
```

### Docker
```dockerfile
FROM node:20-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --production
COPY . .
RUN npm run build
EXPOSE 3001
CMD ["npm", "start"]
```

## ğŸ¯ Architecture

```
backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraping/
â”‚   â”‚   â”œâ”€â”€ fierce-biotech-scraper.ts     [NEW]
â”‚   â”‚   â”œâ”€â”€ science-daily-scraper.ts      [NEW]
â”‚   â”‚   â”œâ”€â”€ biospace-scraper.ts           [NEW]
â”‚   â”‚   â”œâ”€â”€ endpoints-scraper.ts          [NEW]
â”‚   â”‚   â”œâ”€â”€ biopharma-dive-scraper.ts     [NEW]
â”‚   â”‚   â”œâ”€â”€ scraping-manager.ts           [UPDATED]
â”‚   â”‚   â”œâ”€â”€ pubmed-scraper.ts
â”‚   â”‚   â”œâ”€â”€ fda-scraper.ts
â”‚   â”‚   â”œâ”€â”€ clinical-trials-scraper.ts
â”‚   â”‚   â”œâ”€â”€ circuit-breaker.ts
â”‚   â”‚   â”œâ”€â”€ rate-limiter.ts
â”‚   â”‚   â”œâ”€â”€ lru-cache.ts
â”‚   â”‚   â”œâ”€â”€ retry.ts
â”‚   â”‚   â””â”€â”€ index.ts                      [UPDATED]
â”‚   â””â”€â”€ routes/
â”‚       â””â”€â”€ scraping.ts                   [UPDATED +20 routes]
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ test-news-scrapers.ts             [NEW]
â”œâ”€â”€ NEWS_SCRAPER_API.md                   [NEW]
â””â”€â”€ src/scraping/SCRAPING_README.md       [NEW]
```

## ğŸš¦ Health Check

Monitor scraper health in real-time:
```bash
curl http://localhost:3001/api/scraping/health
```

Response includes:
- Circuit breaker states
- Rate limiter status
- Cache statistics
- Error rates
- Last check timestamps

## ğŸ’¡ Best Practices

1. **Use caching**: Most requests are cached for 1-4 hours
2. **Implement client-side rate limiting**: Don't hammer the endpoints
3. **Handle errors gracefully**: Scrapers may be temporarily down
4. **Use the aggregated endpoint** for dashboard views
5. **Monitor health endpoint** for system status

## ğŸ“ License

MIT - Same as parent project

## ğŸ¤ Contributing

See [SCRAPING_README.md](./src/scraping/SCRAPING_README.md) for contribution guidelines.

---

**Built with â¤ï¸ for the biotech community**
